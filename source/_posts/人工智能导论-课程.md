---
title: 人工智能导论[课程]
date: 2023-09-22 10:51:12
tags: [class]
---

# 人工智能的应用

## 图像识别与分类

### 什么是图像识别

一般而言，传统图像识别系统主要由图像分割、图像特征提取以及图像识别分类构成。图像分割将图像划分成为多个有意义的区域，然后将每个区域的图像进行特征提取，最后根据提取的图像特征对图像进行分类。

### ImageNet数据集

是一个人工标定的数据集，包含1400多万张图片，2万多个类别。关于图像分类定位检测等研究大多基于此数据集展开，几乎成为目前深度学习图像领域算法性能检验的标准数据集。

### 卷积神经网络工作过程

1、对输入的真实图像运用不同的图像算子（包括边缘检测等）进行扫描来提取图像当中的不同特征；

2、通过采样对特征进行压缩（防止特征数量过于庞大）

3、进行多次特征提取和采样

4、全连接层为每个节点的输出指定一个标签

### 图像识别的主要作用

智能家居、安防、金融、医疗、交通

#### 医学影像分析



* 通过深度学习和大数据等，在医学影像的基础上，完成对影像的分类目标检测图像分割和检索，帮助医生完成诊断治疗。
* 使用深度学习多层感知器，通过组合低层特征形成更加抽象的高级特征，提取出人体结构。对身体组织做明确分割精度比医生更高，可以更加精准地定量评价治疗前后的效果。
* 图像配准，图像融合，图像重建

#### 语音识别

* 语音拨号、导航、室内设备控制
* 与其他自然语言处理技术结合可以构建更复杂的应用，如同声传译。
* 语音输入
* 语音助理

语音识别的基本原理：
语音分帧  声学特征提取 把帧识别成状态 把状态组合成音素 把音素组合成单词

判断某帧属于哪个状态的概率大（通过巨大数量的语音数据 经过神经网络监督训练得到声学模型 由此判概率）

历史信息是一个序列 可以用递归神经网络RNN来建立声学模型

#### 人脸识别和情感计算

图像采集 人脸检测 特征处理 模型对比 输出结果

图像采集 影响因素 图像分辨率 光照。。。

人脸检测 准确标定人脸位置和大小提取有用的信息  如直方图特征 颜色特征 模板特征 结构特征

基于检测出的特征 采用分类学习算法挑选代表性特征 按照加权的方式将弱分类器构造为强分类器 再将训练的若干强分类器 串联组成层叠分类器 提高检测速度

图像预处理 早期阶段的灰度矫正和噪声过滤

 人脸特征提取 基于神经网络方法 将输入的图像转换成向量的表示 基于传统知识表征方法 通过器官的形象描述和近距离特性来进行特征建模

匹配识别 设定一个阈值 相似度大于阈值则输出



情感计算  人脸检测人脸配准 特征提取 表情分类

考虑精细化 多样化 精细表情识别 混合表情识别 非基本表情识别

#### 人工智能与自动驾驶

* 包括模块， 决策模型：障碍物识别、交通标志识别、车道识别

最重要的就是决策模块 深度学习用作环境感知 强化学习用作控制行为的决策模型

障碍物识别 用传感器融合算法 利用多个传感器获取关于环境全面的信息 融合算法 实现障碍物识别跟踪躲避

交通标志 深度学习 卷积神经网络 

车道识别 神经网络图像处理 

* 典型方案：以摄像头原始图像作为输入利用Autoencoder编码 用循环神经网络RNN从人类驾驶数据中进行转换学习 拟合逼近最优驾驶策略

# 人无法知道机器思考的具体过程

# 挑战

## 技术

* 能否保证人工智能的应用开发被用于正确目标 如果用于错误目标 由于人工智能的强大能力 产生的负面影响可能是快速而大规模的
* 智能系统开发时可能存在缺陷。人无法知道机器思考的具体过程 机器学习数据来源和质量问题可能教坏人工智能
* 人工智能研发者多元化 在制作机器人时 会将自己的想法加入导机器人的思维系统中 从而使很多人工智能应用无法同时使所有用户满意
* 用户不一定有充分使用这些应用  的知识和技能 如何对人工智能的服务群体进行指引和监管 将是一大难题
* 对人工智能服务群体进行指引和监督
* 让机器人和人类一样遵守伦理道德 能否在技术上融入伦理道德 （把道德准则通过算法转换成人工智能的行为准则 让系统做出人类认为正确的道德决策 只是人工智能比人类更理性客观

## 人文

* 对人的认知的冲击

随着人工智能的发展 人们对自身的理解越来越去意义化和物化  如何对待机器与自身的关系 人有超越机器的东西吗 人们该如何应对

* 对人类思维的挑战

* 人工智能高度模拟人的思维方式，让人类失去学习热情

* 对人类社会结构和心理的冲击 机器人是否会取代人类 

  一方面可以为解决贫困和不平等提供方案 

  另一方面可能由于技术壁垒 将一部分人永远的排斥在劳动市场外

  可以推动进一步的专业分化 和创造新的工作机会 但并非所有人都有能力迈过技术性和社会性的壁垒

  未来社会可能产生无用阶级 人对技术进步和快速经济社会转型所产生的无用感和无力感会磋商人的发展信心

* 对隐私保护的冲击，大数据的发展使人们的隐私暴露  合法获取和利用数据也会对隐私产生威胁 大数据揭示人的行为规律 使人前所未有地暴露在他人面前

* 对经济市场的冲击 知识经济时代有条件接触导大量数据的企业会拥有独特的竞争优势 这种垄断对于公共福祉的影响同样值得重视

如何看待 ？

一方面带来极大的边界与财富 另一方面带来很大的社会焦虑 技术的革新无法阻挡 与其逃避问题 不如解决问题

## 社会

人工智能的快车要加上刹车 也就是 伦理规范 只有符合社会伦理规范和公共政策的解决方案 才能设计出可信赖的人工智能

公平安全保密包容是人工智能开发应用的道德原则  而透明和负责是其他四项基本原则的基石

* 就业问题，人类是否会被机器取代

前三次工业革命旧机器取代人的体力劳动  人工智能革命  开始替代人的脑力劳动 这就导致劳动力市场的大变革

* 安全问题，

任何技术长信的最大应用前提都是安全 最大危险是人类对他失去控制 落入不法分子手中

一方面得益于互联网的大数据发展资源优势 另一方面互联网人为因素 的黑客病毒有对人工智能的安全性构成巨大威胁

* 责任于义务问题，人工智能事故由谁负责

通过民法解决 

高校和研究机构开展前瞻性科技伦理 人文 哲学研究

人工智能领域开展广泛对话和持续合作 鼓励企业和行业协会成立科技伦理部门或委员会 

风险投资机构和资本市场鼓励和引导企业进行负责任的产品开发

公共政策

明确人工智能开发应用的政策策略 加大人工智能领域的研究投入 成立人工智能研发和维护的专业协会 加强对人工智能开发应用的伦理和法律监管 促进充分隐私保护和安全前提下的数据共享合作 人工智能安全评估和管理 普及人工智能教育



弱人工智能 单一 强人工智能 全面 

递归的自我改进 一个运行在智能水平的人工智能 一般有自我改进的机制



# 人工智能的定义

类人思维系统 理性思维系统 类人行为系统 理性行为系统 

研究用人工的方法和技术 模拟和拓展人的智能 实现机器智能

长期目标 实现人类水平的智能 模拟或创造智能的目标被分解成很多次级方向

认识和理解世界 推理和决策 学习能力 自我适应能力

 研究应用领域  机器学习 数据挖掘 知识表示推理规划 计算机视觉 自然语言处理 移动和控制 情感计算

# 人工智能基础 

## 机器学习

研究计算机怎样模拟或实现人类的学习行为 以获取新的知识或技能 重新组织已有的知识结构 使之不断改善自身的性能 是人工智能的核心 是计算机具有智能的根本途径

分为

### 监督学习 

通过已有的一部分输入数据与输出数据的对应关系 生成函数 将输入映射到合适的输出 例如分类 回归

### 无监督学习

只给机器训练数据 不给结果（标签） 计算机只能 分析数据特征得到一定成果  通常是一些集合 集合内的数在某些特征上相同或相似 例如聚类

 ### 半监督学习  

是监督学习与无监督学习相结合的一种学习方法 半监督学习的训练一部分带有标签一部分没有标签  无标签的数量远大于有标签的数量 数据分布不完全随机 勇敢一些有标签的数据的局部特征 以及更多没标签的数据的整体分布 进行模式识别工作

### 强化学习

智能体以试错的方式进行学习 通过与环境进行交互获得的奖赏指导行为 目标是使智能体获得最大的奖赏 

做出行动 观察环境 计算收益

## 机器学习方法

传统的机器学习方法有线性回归 逻辑回归 决策树 支持向量机 随机森林 贝叶斯模型 正则化模型 集成学习 神经网络 

### 交叉验证

统计学上将数据样本切割成较小子集的使用方法  将原始数据进行分组 一部分作为训练集 一部分作为验证集 首先用训练集对分类器进行训练 再利用验证集来测试训练得到的模型 以此作为评价分类器的性能指标

常见的一种形式是K折交叉验证 初始采样分割成K个子样本 一个单独的子样本被保留为验证模型的数据 其他K-1个样本用来训练

## 人工神经网络

仿生物神经网络 结构和功能的 数学模型或计算模型  用于对函数进行估计或近似  经典的包括输入层输出层隐藏层

有大量神经元相互连接而成 每个神经元接受线性组合的输入后  最开始只是简单的线性加权 后来给每个神经元加上了非线性的激活函数 进行非线性变换后输出  每两个神经元之间 连接代表加权值 称之为权重 不同的权重和激活函数 会导致神经网络的不同输出

神经元模型是一个包含输入输出计算功能的模型 输入可以类比为神经元的树突 输出可以类比为神经元的轴突 计算可以类比为细胞核 

## 深度学习

 是机器学习的分支 动机在于建立模拟人脑进行分析学习的更复杂的神经网络 模仿人脑的机制来解释数据 例如图像声音和文本 深度学习在自然语言处理  图像识别语音识别语音合成 机器翻译等领域取得显著成果

听说 - 语音识别 语音合成

交流读写- NLP

看-CV

### 语音识别

人的语音内容转换成相应的文字 大多采用混合高斯模型GMM来描述每个建模单元的统计概率模型 实质上是浅层学习网络建模 状态的特征不能被充分描述   后 基于深度学习网络的语音识别系统

### 语音合成

 将一般语言的文字转换成语音 

### CV

用摄影机和计算机代替人眼对目标进行识别 葛总测量等机器视觉 并进一步左图像处理 用计算机处理成为更适合人眼观察或传送给仪器检测的图像 包括图象识别 场景重现图像恢复等

### NLP

人与机器之间使用自然语言进行有效通信的各种理论和方法 包括句法语义分析 信息抽取 文本挖掘 机器翻译 信息检索 问答系统 对话系统 

## 循环神经网络RNN

相较于静态神经网络 RNN增加了循环回馈部分 即状态记忆功能  在处理当前输入时 将历史状态作为输入 即对历史的记忆 行为模式更加接近人脑 传统RNN处理长期记忆力不从心LSTM解决了这个问题

## 卷积神经网络CNN

是一种前馈神经网络  他的人工神经元可以相应一部分覆盖范围内的周围单元 对于大型图像处理效果好 由一个或多个卷积和顶端的全连接层组成 同时也包含关联权重和池化层 使之可以 利用输入数据的二维结构

重要操作 卷积 和池化  卷积将一个二维矩阵转换成另一个更小的二维矩阵 降低网络模型复杂度 减少了神经网络需要训练参数的个数

### 基于CNN的艺术风格图像生成

由多层组成 每层是一些小计算单元 将每层计算单元理解成图像过滤器的集合  每个图象过滤器从输入图像提取一些特征  该卷积的高层提取抽象特征 用于输入图像的风格表示  低层提取具象特征 用于输入图像的内容表示

### 生成对抗网络GAN

两个神经网络通过相互博弈的方式进行学习 由生成器和判别器组成   生成器从潜在空间随机采样输入  输出结果尽量模仿训练集中的真实样本 判别器输入为真是样本或生成网络的输出  目的将生成网络的输出从真是样本中尽可能分离出来  而生成网络 要尽可能欺骗判别网络 两个网络相互对抗不断调整参数 最忠实的判别网络无法判别

### 创意对抗网络CAN

损失函数在生成对抗网络上加了多标签交叉熵损失 避免生成与现有数据过于类似的内容

# 类脑计算

认知科学涉及对认知过程的研究，这包括知觉、思考、记忆、语言理解等。人工智能脑和认知科学的结合旨在创造更智能、更自适应的系统，使机器能够更自然地与人类进行交互和理解人类需求。

机器人伦理是研究人工智能系 统在社会中的道德和伦理问题的领域。这包括确保人工智能系统的使用是公正和透明的，避免对社会产生负面影响。伦理问题可能涉及隐私、歧视、安全等方面，需要制定相关政策和法规来规范人工智能的发展和使用。

在认知科学中，认知是指获取、处理和理解信息的能力。这涉及到感知、思考、解决问题等一系列心理过程。认知研究帮助我们了解大脑如何处理信息，从而对人工智能的发展提供启示。

记忆是认知过程中的一个重要方面，涉及信息的存储和检索。人工智能系统也在努力模拟这种记忆功能，以便更好地处理复杂的任务和适应不断变化的环境。

注意力是认知过程中的另一个关键元素，指的是集中精力在特定信息或任务上的能力。在人工智能中，设计具有选择性注意力的系统可以使其更有效地处理信息，类似于人类在面对复杂环境时的注意力分配能力。

# AI艺术的定义

捕获一定内容，通过机器学习或深度学习将其与某类风格进行融合 从而形成迁移

AI没有先验数据缺乏独创能力，本质上是对海量作品积累形成的数据，没有灵感迸发没有主客体审美 无法处理即生信息 作品缺乏惊喜

具有强烈的失焦 是散点的一种表达 除了人类的第一人称视角 还具有第二人称和第三人称视角 在多种视角之下 每个点都可能成为焦点 也都可能不成为焦点

AI艺术的精髓就是惊奇计算 将人类对01N之外的无极之境 突破想象力 人文艺术-社会科学文化-自然科学文化 

而AI艺术恰恰突破 0以外  

最开始AI艺术从CNN卷积神经网络产生风格的迁移再到GAN生成对抗网络的突破 再通过AI CAN创意对抗网络生成不一样的画风

CAN的目标是希望生成的作品能与现有的作品 产生风格度的区分



AI赋能文化遗产 保护和传承

保护 数字视觉监测 AI安防预警

# 人类艺术审美的特点



具有个体差异性；

受到社会环境、文化和传统的深刻影响，是随着认知和思想不断发展变化的；

是人类的内在想法的主观表达，具有个人喜好，受到情感影响

具有创新性，具有自然性。

# 人类审美和AI审美的异同

## 不同点

* 认知和情感

人类审美是高度主观的，基于每个个体的情感，经验和文化背景的影响。

而AI审美是基于算法和数据的分析，没有情感和主观的偏好

* 学习方式

人类审美是通过学习和经验基类形成的，通过观察和感知各种艺术作品来培养审美感知

AI审美通过机器学习等算法实现，通过训练数据学习，生成特定的审美评估

## 相同点

* 学习方式

都需要通过数据进行感知和学习，在学习中不断丰富和更新审美

* 思考

都可以对数据进行分析，最终得出审美评价。



GPT，或称为“生成式预训练模型”（Generative Pre-trained Transformer），是一种人工智能模型，采用了Transformer架构，通过大规模的文本数据预训练而来。GPT的主要功能是自然语言处理（NLP），它可以用于各种文本生成和理解任务。

GPT的主要功能包括：

1. 文本生成：GPT可以生成文本，包括文章、故事、新闻摘要、诗歌等。它能够根据输入的文本或主题生成连贯、自然的文本内容。
2. 语言理解：GPT可以理解文本，包括自然语言问答、文本分类、情感分析等任务。它可以分析文本并提供相关信息或情感的解释。
3. 机器翻译：GPT可以用于自动翻译文本，将一种语言翻译成另一种语言，实现跨语言沟通。
4. 对话系统：GPT可以用于构建聊天机器人或虚拟助手，使其能够进行自然语言对话，并回应用户的问题和需求。
5. 文本摘要：GPT可以自动生成文本摘要，将长篇文章或文档压缩成简短的摘要，提供重要信息的概述。
6. 文本生成任务的个性化：GPT可以根据输入的上下文和特定要求生成个性化的文本，例如定制化推荐信、创作故事结局等。
7. 自动代码生成：一些变种的GPT模型，如GPT-3，可以用于生成编程代码，帮助程序员自动完成常见编码任务。
8. 媒体内容生成：GPT可以生成图片描述、视频字幕、广播稿件等各种媒体相关的内容。
9. 内容审核：GPT可以用于检测和审核文本内容，识别不当内容或违规言论。
10. 领域特定应用：GPT可以通过微调适应特定领域的需求，如医疗保健、法律、金融等。

GPT模型在NLP领域广泛应用，因为它在大规模预训练的基础上，能够适应各种文本生成和理解任务，并在很多应用中取得了出色的性能。不过，需要注意的是，GPT模型需要大量的计算资源和数据进行训练和微调，以获得最佳的性能。



# 神经元和神经网络

## 人工神经元和神经网络

输入信号 x1,x2..xn

对输入信号进行加权处理确定强度 对输入信号求和确定组合效果 通过激励函数确定输出

输出信号y1 y2...

人工神经网络模仿神经元在人脑中的结构

一个完整的神经网络由一层输入层多层隐藏层一层输出层构成



## 前馈型神经网络

单项多层结构 各层神经元从输入开始 只接受上层输出并输出到下一层 直至输出层 整个网络无反馈

常用于图像识别检测分割

## 反馈型神经网络

从输出到输入有反馈连接的神经网络 当前的结果受先前所有结果的影响

常用于语音文本处理 问答系统

## 训练算法

以误差为主导的反向传播算法（BP算法）

本质是通过前向传递的输入信号直至输出信号产生误差 再将误差信息反向传播去更新网络权重矩阵

通过这种反馈机制 反馈越多 学习结果越准确（识别手写字体正确率99.8  imageNet图像分类优于人类）

# 手写数字识别过程

MNIST是一个著名的手写数字识别数据集 

训练集类似于学习资料 提升学习能力

测试集类似于考卷 检验学习成果

标签是人为给定 类似于参考答案

样本为 0~9的数字的灰度图片 对应一个代表数字的标签  图片大小28*28 数字出现在正中间

电脑得到一系列像素点的灰度值数据 

(向量就是多个数字按顺序排成一组 数字的个数成为向量的维数  )

每个样本图像的输入都是一个784维的向量 

每个标签数据中把数字n表示为一个只有在第n维数字为1的10维向量

如2[0,0,1,0,0,0,0,0,0,0]

输入的784维向量经过两层中间计算 输出10维向量 每个维的值代表该数字的概率

## 隐藏层

相当于人类大脑提取特征的各个神经层 从一系列数据中提取特征 需要不同的隐藏层来实现

隐藏层包括卷积层 全连接层 池化层 归一化数据层 激活层等

通过多个顺序链接的隐藏层的组合 神经网络就可以讲原始图像便化成高层次的抽象的图像特征 从而认识其中的东西

### 卷积层

提取图像的二维特征 通过不同的算子检测图像的不同边缘

（垂直边缘算子-垂直边缘卷积层）

### 全连接层

用来将所有的特征融合到一起  把计算得到的蕴含特征的大维度向量转换成 与输出层维度相同  每个点都与下一层链接

### 池化层

减少训练参数 对原始特征信号进行采样

输入数据过大时卷积层计算量过大 这是需要减少参数 （通常出现在卷积层之后）

### 激活层

非线性激活层就是决定哪些神经元活跃程度高 哪些低

ReLU线性整流函数

### 归一化指数层

完成最后输出分类时 类别概率的计算 使得最后向量每个值加起来总和为1 每个单独的数值就是AI认为图像是该类别的概率

## 特征提取

手写数字识别采用两层隐藏层来对图像特征进行提取 每一层以前一次提取作为输入 得到更复杂的特征



## 损失函数

用带标签的样本数据进行学习 成为监督学习

把输出与标签比对判断预测的准确性 用损失函数来定量计算

损失函数 又叫代价函数 评估模型预测值与真实值误差大小

神经网络训练过程就是最小化损失函数 不断优化网络参数 时预测更接近真实值

0-1损失函数  （常用于分类 属于i是1 不属于i是0

平方损失函数 L = (Y - f(x))^2

Y 真实值     F(x) 预测值

## 优化器

在确定损失函数后 优化网络交给优化器 

优化器代表调整网络参数 使损失函数达到最小的过程 不同优化器有不同算法 改变损失函数的过程和效率也不同

能否达到损失函数最小的地方？可行性分析

能否最快的达到？效率分析

### 梯度

函数在某点变化最快的方向

梯度下降算法  找到损失函数下降最快的地方进行迭代

问题  可能会找到次优解 即局部最优解  而陷入局部最优之后 就难以跳出 而找不到全局最优  可以借助其他算法或进行认为调整

不一定要 找到全局最优  只需要找到令人满意的误差

## 反向传播算法（BP）

根据梯度下降的算法 将当前损失函数反馈给之前各层神经网络 并调整各层神经网络参数的权值 这个过程成为反向传播

得到了不理想的结果 不能直接去调整这个结果 而是要去调整权值参数  使其调整后计算的结果接近正确结果

一个神经元的输入输出类比多元一次方程

y = w1x1 + w2x2 + b

通过调整wxb来调整y 优先选择梯度大的进行调整（对y影响大）

w 调整激活值大的神经元的参数 较好

x 增加原始参数w使正值的神经元的激活值 （幅度与w有关）

无法直接改变每个神经元的激活值 于是       输出需要改变的大小 叠加之后 能够得到的 总变量（假设当前为输出层 损失函数误差过大 需要反向传播   就需要改变上一层的每个神经元的w  还需要调整激活值  再把总的改变量累加到一起  这样就有一个对前一层的期望总改变量   这就实现了最后一层到倒数第二层的改变量）然后不断向前传播  就是反向传播 



## 归纳神经网络深度学习的要素和整个过程

要素 数据

神经网络深度学习的过程：输入原始数据--特征提取（卷积 全连接）--采样（池化）--活跃度（激活）--输出分类概率计算（归一化）--输出

神经网络使深度学习的理论基础 人工神经元和神经网络都模仿人脑的神经结构 旨在让计算机有更强大的学习能力 神经网络主要学习机制为误差反向传播算法  不需要固定程序去学习的都比较适合深度学习

## 为什么说图像分类适合前向型 语音识别和自然语言处理适合反馈型

前馈型神经网络取连续或离散变量 一般不考虑输出和输入再时间上的滞后效应 只表达输入与输出的映射关系；反馈型神经网络可以用离散变量也可以用连续取值 考虑输入输出的时间延迟 需要动态方程描述系统的模型

前馈型神经网络主要用误差修正算法 如BP计算 收敛较慢  反馈型神经网络主要用Hebb学习规则 一般收敛速度块  

反馈网络也有类似前馈的应用并且再联想记忆和优化计算方面的应用更显特点

# 作业内容



### 什么是训练集

训练集是机器学习模型用于训练和学习的数据集 通常情况下是原始数据集的一部分 用于训练模型的参数 包含了已知的标签

类似于学习资料 提升学习能力

### 什么是测试集

测试集用来检验最终选择出来的最优模型的性能如何 通常用来检验模型的准确度和泛化能力

类似于考卷 检验学习成果

### 什么是损失函数

损失函数 又叫代价函数 是用来度量神经网络的性能的指标 评估模型预测值与真实值误差大小

神经网络训练过程就是最小化损失函数 不断优化网络参数 使预测更接近真实值

### 手写数字识别的过程

#### 数据准备

导入MNIST数据集 （每个图像已经做好相应的标签）

对图像进行预处理 转化成28*28的矩阵

#### 搭建网络 & 训练模型

搭建神经网络 其中包含输入层 输出层和隐藏层 

将输入的图片的转换成灰度值的矩阵 输入到输入层 做全连接线性计算 再套上激活函数传递给下一层 直到输出层 

使用训练集进行训练 通过BP算法来不断更新参数 调整权重和偏置

在每个epoch中计算损失函数的值  然后使用优化器最小化损失

#### 模型评估

训练结束后  用测试集测试  发现模型测试准确率高

除了MNIST的测试集 还加入了自己手写的图片 测试效果良好



以下为基于pytorch实现的手写数字识别代码

```py
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
import matplotlib.pyplot as plt
from PIL import Image

transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,),(0.3081,))]
)

class Net(torch.nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(28 * 28, 64)  # 输入 28 * 28像素的图像
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, 64)  # 2个隐层
        self.fc4 = torch.nn.Linear(64, 10)  # 输出10维向量

    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(
            self.fc3(x))  # 前三层 每层先做全连接线性计算 即self.fc1(x) 再套上整流激活函数ReLU（torch.nn.functional.relu）
        x = torch.nn.functional.log_softmax(self.fc4(x), dim=1)  # 输出层通过softmax归一化（log_softmax是为了提高计算稳定性）
        return x


# 转换数据
def get_data_loader(is_train):
    to_tensor = transforms.Compose([transforms.ToTensor()])  # tensor就是多维数组 也叫张量 这行是转换数据类型为高维向量
    data_set = MNIST("", is_train, transform=to_tensor, download=True)  # （“”下载目录 空表示当前目录 is_train表示输入训练集还是测试集）
    return DataLoader(data_set, batch_size=15, shuffle=True)  # batch_size 表示一次输入15个数据 shuffle表示数据是随机打乱的  最后返回数据加载器


# 评估识别正确率
def evaluate(test_data, net):
    n_correct = 0
    n_total = 0
    with torch.no_grad():
        # 从测试集中按批次取出数据
        for (x, y) in test_data:
            outputs = net.forward(x.view(-1, 28 * 28))  # 算神经网络的预测值
            # 对批次中的每个结果进行比较 累加正确预测的数量
            for i, outputs in enumerate(outputs):
                if torch.argmax(outputs) == y[i]:  # argmax是算数列中最大值的序号
                    n_correct += 1
                n_total += 1
    return n_correct / n_total  # 正确率


# 主函数
def main():
    # 初始化神经网络
    train_data = get_data_loader(is_train=True)
    test_data = get_data_loader(is_train=False)
    net = Net()
    # 训练前 打印 初始网络正确率（大概1/10）
    print("initial accuracy:", evaluate(test_data, net))
    """
        以下为pytorch的固定写法
        for epoch：（反复训练几次）
            for 数据：
                初始化
                正向传播
                计算损失函数
                反向误差传播
                优化网络参数
            print（每个轮次结束后打印当前网络正确率 一般越来越高）
    """
    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
    for epoch in range(2):
        for (x, y) in train_data:
            net.zero_grad()
            output = net.forward(x.view(-1, 28 * 28))
            loss = torch.nn.functional.nll_loss(output, y)  # 采用nll_loss 对数损失函数 是为了匹配上面的log_softmax对数
            loss.backward()
            optimizer.step()
        print("epoch", epoch, "accuracy:", evaluate(test_data, net))

    # 随机抽取三张图像 做测试
    for (n, (x, _)) in enumerate(test_data):
        if n > 3:
            break
        predict = torch.argmax(net.forward(x[0].view(-1, 28 * 28)))
        plt.figure(n)
        plt.imshow(x[0].view(28, 28))
        plt.title("prediction: " + str(int(predict)))
    plt.show()


if __name__ == "__main__":
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    main()


```

![image-20231030232144091](../images/$%7Bfiilename%7D/image-20231030232144091.png)

可以发现 无论是两次循环的计算的准确率 还是实测的准确率都很高
